{"name":"Libcmaes","tagline":"libcmaes is a multithreaded C++11 library for high performance blackbox stochastic optimization using the CMA-ES algorithm for Covariance Matrix Adaptation Evolution Strategy","body":"## libcmaes\r\nlibcmaes is a multithreaded C++ implementation of the CMA-ES algorithm for stochastic optimization of nonlinear 'blackbox' functions. The implemented algorithms have a wide range of applications in various disciplines, ranging from pure function minimization, optimization in industrial and scientific applications, to the solving of reinforcement and machine learning problems.\r\n\r\nOver the past decade, both the original CMA-ES and its improved flavors have proven very effective in optimizing functions when no gradient is available. Typically, the algorithm does find the minimum value of an objective function in a minimal number of function calls, compared to other methods. For a full report of recent results, see (3).\r\n\r\nCMA-ES is mostly the work of Nikolaus Hansen (4) and a few others. Other implementations can be found in (5).\r\n\r\nMain functionalities:\r\nAt the moment, the library implements a vanilla version of CMA-ES (1).\r\nCurrent features include:\r\n\r\n- high-level API for simple use in external applications;\r\n- implements several flavors of CMA-ES, IPOP-CMA-ES, BIPOP-CMA-ES, active CMA-ES, active IPOP and BIPOP restart strategies, sep-CMA-ES (linear time & space complexity) along with support for IPOP and BIPOP flavors as well;\r\n- some operations benefit from multicores;\r\n- support for objective function gradient, when available;\r\n- a control exe in the command line for running the algorithm over a range of classical single-objective optimization problems.\r\n\r\nDependencies:\r\n\r\n- [eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) for all matrix operations;\r\n- [glog](https://code.google.com/p/google-glog/) for logging events and debug (optional);\r\n- [gflags](https://code.google.com/p/gflags/) for command line parsing (optional);\r\n- [gtest](https://code.google.com/p/googletest/) for unit testing (optional).\r\n\r\nImplementation:\r\nThe library makes use of C++ policy design for modularity, performance and putting the maximum burden onto the compile-time checks. The implementation closely follows the algorithms described in (2) and (6).\r\n\r\n### Authors\r\nlibcmaes is designed and implemented by Emmanuel Benazera on behalf of Inria Saclay / Research group TAO / LAL Appstats.\r\n\r\n### Build\r\nBeware of dependencies, typically on Debian/Ubuntu Linux, do:\r\n\r\n```\r\nsudo apt-get install libgoogle-glog-dev libgflags-dev libeigen3-dev\r\n```\r\n\r\nFor compiling with basic options enabled:\r\n```\r\n./autogen.sh\r\n./configure\r\nmake\r\n```\r\n\r\n### Run examples\r\n```\r\ncd tests\r\n./test_functions --dim 30 --lambda 100 --max_iter 120 --fname fsphere\r\n```\r\nto minimize the sphere function in 30D with 100 offsprings per generation,\r\n```\r\n./test_functions --dim 20 --lambda 100 --max_iter 1000 --fname rosenbrock\r\n```\r\nto minimize the Rosenbrock function in 20D with 100 offsprings. To see available function, do\r\n```\r\n./test_functions --list\r\n```\r\nto plot results, use a file output and then the included Gnuplot script\r\n```\r\n./test_functions --fname rastrigin --dim 10 --lambda 200 --max_iter 130 --fplot out.dat -sigma0 5 -x0 5 -seed 5489\r\ngnuplot -e \"filename='out.dat'\" cma_multiplt.dem\r\n```\r\nto plot results with matplotlib instead\r\n```\r\npython cma_multiplt.py out.dat\r\n```\r\nto run a check across a range of classical single-objective optimization functions:\r\n```\r\n./test_functions --all\r\n```\r\nfor help, do\r\n```\r\n./test_functions --help\r\n```\r\n\r\n### Sample code\r\n\r\n```C++\r\n#include \"cmaes.h\"\r\n#include <iostream>\r\n\r\nusing namespace libcmaes;\r\n\r\nFitFunc fsphere = [](const double *x, const int N)\r\n{\r\n  double val = 0.0;\r\n  for (int i=0;i<N;i++)\r\n    val += x[i]*x[i];\r\n  return val;\r\n};\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n  int dim = 10; // problem dimensions.                                                                    \r\n  //int lambda = 100; // offsprings at each generation.\r\n  //CMAParameters cmaparams(dim,lambda);\r\n  CMAParameters<> cmaparams(dim);\r\n  //cmaparams._algo = BIPOP_CMAES;                                                                        \r\n  CMASolutions cmasols = cmaes<>(fsphere,cmaparams);\r\n  std::cout << \"best solution: \" << cmasols << std::endl;\r\n  std::cout << \"optimization took \" << cmasols._elapsed_time / 1000.0 << \" seconds\\n\";\r\n  return cmasols._run_status;\r\n}\r\n```\r\n\r\n### Practical hints\r\n\r\nCMA-ES requires two components from the user:\r\n- the initial start point x0;\r\n- the initial value for sigma, the so-called step-size or error guess.\r\n\r\nIn short: the optimum that is looked after should better not be far away from the interval [x0 - sigma0, x0 + sigma0] in each dimension, where distance is defined by sigma0.\r\n\r\nSee https://www.lri.fr/~hansen/cmaes_inmatlab.html#practical for more detailed useful advices using CMA-ES.\r\n\r\n### Run BBOB 2013 Black-Box Optimization Benchmark\r\n\r\nThere's an install script in the repository. Do:\r\n```\r\ncd tests\r\n./bbobsetup.sh\r\n```\r\nyou can now benchmark any of the implemented flavors of CMA-ES (beware, this make take a while, ~hours):\r\n```\r\n./bbobexperiment -alg bipop\r\n```\r\nfor the command above, results will be in repository bipop_bbob\r\nSee (7) for more information and details.\r\n\r\n### References\r\n- (1) Hansen, N. and A. Ostermeier (2001). Completely Derandomized Self-Adaptation in Evolution Strategies. Evolutionary Computation, 9(2), pp. 159-195.\r\n- (2) Hansen, N. (2009). Benchmarking a BI-Population CMA-ES on the BBOB-2009 Function Testbed. Workshop Proceedings of the GECCO Genetic and Evolutionary Computation Conference, ACM, pp. 2389-2395. http://hal.archives-ouvertes.fr/inria-00382093/en\r\n- (3) N. Hansen, A. Auger, R. Ros, S. Finck, P. Posik: Comparing Results of 31 Algorithms from the Black-Box Optimization Benchmarking BBOB-2009. Workshop Proceedings of the GECCO Genetic and Evolutionary Computation Conference 2010, ACM. http://www.lri.fr/~hansen/ws1p34.pdf\r\n- (4) https://www.lri.fr/~hansen/\r\n- (5) https://www.lri.fr/~hansen/cmaes_inmatlab.html\r\n- (6) Hansen, N., R. Ros (2010). Benchmarking a Weighted Negative Covariance Matrix Update on the BBOB-2010 Noiseless Testbed. Workshop Proceedings of the GECCO Genetic and Evolutionary Computation Conference 2010, ACM, pp. 1673-1680, https://www.lri.fr/~hansen/ws1p32-hansen.pdf\r\n- (7) http://coco.gforge.inria.fr/doku.php?id=bbob-2013\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}